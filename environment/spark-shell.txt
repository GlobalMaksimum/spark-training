spark 3_2.13: docker run --rm --name sparkshell -it --network environment_default -p 4040:4040 -v `pwd`/data/small:/data capacman/spark-shell:3.2.1 spark-shell --master spark://spark-master:7077

docker run --rm --name sparkshell -it --network environment_default -p 4040:4040 -v `pwd`/data/small:/data -v `pwd`/app:/app capacman/spark-shell:3.2.1  spark-shell --master spark://spark-master:7077 --packages io.delta:delta-core_2.13:1.2.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" --conf spark.hadoop.hive.metastore.uris=thrift://metastore:9083 --conf spark.sql.warehouse.dir=/data/warehouse --conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict
docker run --rm --name sparkshell -it --network environment_default -p 4040:4040 -v `pwd`/data/small:/data -v `pwd`/app:/app capacman/spark-shell:3.2.1  spark-shell --master spark://spark-master:7077  --packages org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.13.1 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog --conf spark.sql.catalog.spark_catalog.type=hive --conf spark.hadoop.hive.metastore.uris=thrift://metastore:9083 --conf spark.sql.warehouse.dir=/data/warehouse --conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict
docker run --rm --name sparkshell -it --network environment_default -p 4040:4040 -v `pwd`/data/small:/data -v `pwd`/app:/app capacman/spark-shell:3.2.1_2.12  spark-shell --master spark://spark-master:7077 --conf spark.hadoop.hive.metastore.uris=thrift://metastore:9083 --conf spark.sql.warehouse.dir=/data/warehouse --conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict